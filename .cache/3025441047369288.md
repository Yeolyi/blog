
## 출처

[MIT OpenCourseWare - Introduction to Algorithms](https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-spring-2020/)

교재: [Introduction to Algorithms](https://ko.wikipedia.org/wiki/Introduction_to_Algorithms). CLRS라고도 한다.

## Course Description

기초적인 자료 구조(동적 배열, 힙, 균형잡힌 이진 탐색 트리, 해시 테이블)과 고전적인 문제 해결을 위한 알고리즘(정렬, 그래프 탐색, 다이나믹 프로그래밍)을 다룬다.

이들 문제에 대한 mathematical modeling을 소개한다.

알고리즘과 프로그래밍간 관계를 강조하며 성능 측정과 분석 기술을 배운다.

## 1. Algorithms and Computation

강의의 목표는 computation problem을 해결하고 해결책이 올바르고 효율적임을 커뮤니케이팅하는 것.

> A **problem** is a binary relation connecting problem inputs to correct outputs.

입력을 종종 문제의 instance라고도 한다.

랜덤 함수는 그럼 알고리즘이 아닌가??

모든 입력에 대한 출력을 명시하면 너무 많으므로 올바른 출력이 만족시켜야 할 verifiable predicate(a property)를 제공한다.

> A (deterministic) **algorithm** is a procedure that maps inputs to single outputs.

알고리즘의 타당성을 증명하기. 작은 입력은 case analysis하면 된다. 커다란 입력을 한정된 크기의 코드로 처리하기 위해서는 **재귀**나 루프를 사용해야하고 따라서 알고리즘의 올바름을 증명하기 위해 귀납법을 사용하게 된다.

알고리즘의 효율성은 하드웨어에 종속적이지 않도록 fixed-time operation의 개수를 세서 측정한다. Asynmtotic performance. 입력의 크기와 무관한 다른 변수가 효율성 측정에 영향을 주지 않도록한다.

> O Notation: Non-negative function g(n) is in O(f(n)) if and only if there exists a positive real number c and positive integer n0 such that g(n) ≤ c · f(n) for all n ≥ n0.

보통 g(n) equel to O(f(n))이라고 말하지만 g(n) ∈ O(f(n))이 더 정확한 표현이다.

> Ω Notation: Non-negative function g(n) is in Ω(f(n)) if and only if there exists a positive real number c and positive integer n0 such that c · f(n) ≤ g(n) for all n ≥ n0.

> Θ Notation: Non-negative g(n) is in Θ(f (n)) if and only if g(n) ∈ O(f (n)) ∩ Ω(f (n)).

Upper bounds (O), lower bounds (Ω), tight bounds (Θ)

Exponential인 경우를 2^(Θ^(n^c))로 쓰네?

보통 입력의 크기가 n이지만, 그래프를 다룰 때에는 Θ(|V| + |E|)이고 행렬을 다룰떄는 Θ(n^2)이다.

알고리즘에 사용되는 자원을 계산하기 위해서는 컴퓨터가 기본 연산을 수행하는데 얼마나 걸리는지 모델링해야한다. 이러한 연산을 모아 model of computation이 만들어진다.

수업에서는 Word-RAM 모델을 사용한다. memory와 processor로 구성된 컴퓨터. Machine word는 w비트의 시퀀스. Word-RAM 프로세서는 사칙연산, 모듈러, 비트 연산등을 두 machine word로 수행할 수 있다.

word는 cpu가 메모리에서 한번에 꺼내올 수 있는 데이터의 양. 요즘 컴퓨터에서는 64비트.

Recitation 5 페이지 주석은 또 읽어보기.

[Model of computation](https://en.wikipedia.org/wiki/Model_of_computation)

[Word RAM](https://en.wikipedia.org/wiki/Word_RAM)

> A data structure is a way to store non-constant data, that supports a set of operations.

> The set of operations supported by a data structure is called an interface

[스털링 근사](https://ko.wikipedia.org/wiki/스털링_근사). 나중에 practice problems에도 나오니 기억해두기.

## 2. Data Structures and Dynamic Arrays

자료 구조는 데이터를 저장하는 방법 및 해당 데이터를 처리하는 알고리즘(support operations on the data)을 의미한다.

가능한 처리 방법을 인터페이스(혹은 API, ADT)라 한다. 인터페이스는 명세, what, 자료 구조는 표현, how.

> Sequences maintain a collection of items in an **extrinsic** order, where each item stored has a **rank** in the sequence.

여기서 extrinsic하다는 것은 요소가 그런 속성을 가지는게 아니라 external party가 그 순서로 요소를 배치했다는 뜻이다.

시퀀스는 스택과 큐의 일반화이다. 이 둘은 시퀀스 작업의 부분 집합을 제공한다.

build(X), len, iter_seq, get_at, set_at, insert_at, delete_at, insert_first, delete_first, insert_last, delete_last.

> Sets maintain a collection of items based on an **intrinsic** property involving what the items are, usually based on a unique **key**.

Set은 딕셔너리나 다른 쿼리 데이터베이스의 일반화이다. 딕셔너리는 order operation이 없는 set이다.

시퀀스 인터페이스는 배열, 연결 리스트, 동적 배열 자료구조로 구현할 수 있다.

build, len, find, insert, delete, iter_ord, find_min, find_max, find_next, find_prev.

운영체제는 각 프로세스에게 고정된 메모리 청크들을 할당한다.

```py
class Array_Seq:
    def __init__(self):
        self.A = []
        self.size = 0

    def __len__(self):
        return self.size

    def __iter__(self):
        yield from self.A

    def build(self, X):
        self.A = [a for a in X]  # 정적 배열을 만든다고 속아주자
        self.size = len(self.A)

    def get_at(self, i):
        return self.A[i]

    def set_at(self, i, x):
        self.A[i] = x

    def _copy_forward(self, i, n, A, j):
        for k in range(n):
            A[j + k] = self.A[i + k]

    def _copy_backward(self, i, n, A, j):
        for k in range(n - 1, -1, -1):
            A[j + k] = self.A[i + k]

    def insert_at(self, i, x):
        n = len(self)
        A = [None] * (n + 1)
        self._copy_forward(0, i, A, 0)
        A[i] = x
        self._copy_forward(i, n - i, A, i + 1)
        self.build(A)

    def delete_at(self, i):
        n = len(self)
        A = [None] * (n - 1)
        self._copy_forward(0, i, A, 0)
        x = self.A[i]
        self._copy_forward(i + 1, n - i - 1, A, i)
        self.build(A)
        return x

    def insert_first(self, x):
        self.insert_at(0, x)

    def delete_first(self):
        return self.delete_at(0)

    def insert_last(self, x):
        self.insert_at(len(self), x)

    def delete_last(self):
        return self.delete_at(len(self) - 1)

    def __repr__(self) -> str:
        return ", ".join(map(str, self.A))
```

Linked list에서는 요소를 저장하기 위해 연속된 청크를 할당받는 것이 아니라 item과 next로 이루어진 node에 요소를 저장한다. 이러한 자료구조는 pointer-based, linked라고도 불린다.

```py
class Linked_List_Node:
    def __init__(self, x):
        self.item = x
        self.next = None

    def later_node(self, i):
        if i == 0:
            return self
        assert self.next
        return self.next.later_node(i - 1)


class Linked_List_Seq:
    def __init__(self):
        self.head = None
        self.size = 0

    def __len__(self):
        return self.size

    def __iter__(self):
        node = self.head
        while node:
            yield node.item
            node = node.next

    def build(self, X):
        for a in reversed(X):
            self.insert_first(a)

    def get_at(self, i):
        node = self.head.later_node(i)
        return node.item

    def set_at(self, i, x):
        node = self.head.later_node(i)
        node.item = x

    # insert_at를 활용해 구현하지 않음을 유의
    def insert_first(self, x):
        new_node = Linked_List_Node(x)
        new_node.next = self.head
        self.head = new_node
        self.size += 1

    def delete_first(self):
        x = self.head.item
        self.head = self.head.next
        self.size -= 1
        return x

    # head가 없는 경우도 고려되어야한다.
    def insert_at(self, i, x):
        if i == 0:
            self.insert_first(x)
            return
        new_node = Linked_List_Node(x)
        node = self.head.later_node(i - 1)
        new_node.next = node.next
        node.next = new_node
        self.size += 1

    def delete_at(self, i):
        if i == 0:
            return self.delete_first()
        node = self.head.later_node(i - 1)
        x = node.next.item
        node.next = node.next.next
        self.size -= 1
        return x

    def insert_last(self, x):
        self.insert_at(len(self), x)

    def delete_last(self):
        return self.delete_at(len(self) - 1)

    def __repr__(self) -> str:
        ret = []
        node = self.head
        while node:
            ret.append(str(node.item))
            node = node.next
        return ", ".join(ret)
```

O(i)인 작업과 O(n)인 작업이 있음 기억하기.

**Dynamic array sequence**. 파이썬 리스트 append는 worst-case O(1)의 시간 복잡도가 아니다. 평균적으로 O(1)이며 이러한 asymptotic running time을 **amortized constant time**이라 한다. 재할당에 O(n)이지만 O(n)개의 operation에서 한번씩하니 평균적으로 O(1). 비용을 여러 operation에 분산시킨다.

Fill ratio, r이란 용량을 차지한 요소의 비중.

두 배씩 한다면 n = 1, 2, 4, 8, 16...에서 resize한다. resize cost는 Θ(1 + 2 + 4 + ...) = Θ(i=1~lgn 2^i)

이를 위해 추가적인 공간을 할당할 때 저장된 요소의 개수에 비례하는 만큼 할당한다. 두 배인 경우 table doubling이라 한다.

```py
from Array_Seq import Array_Seq


class Dynamic_Array_Seq(Array_Seq):
    def __init__(self, r=2):
        super().__init__()
        self.size = 0
        self.r = r
        self._compute_bounds()
        self._resize(0)

    def __len__(self):
        return self.size

    def __iter__(self):
        for i in range(len(self)):
            yield self.A[i]

    def build(self, X):
        for a in X:
            self.insert_last(a)

    def _compute_bounds(self):
        self.upper = len(self.A)
        self.lower = len(self.A) // (self.r * self.r)

    def _resize(self, n):
        if self.lower < n < self.upper:
            return
        m = max(n, 1) * self.r
        A = [None] * m
        self._copy_forward(0, self.size, A, 0)
        self.A = A
        self._compute_bounds()

    def insert_last(self, x):
        self._resize(self.size + 1)
        self.A[self.size] = x
        self.size += 1

    def delete_last(self):
        self.A[self.size - 1] = None
        self.size -= 1
        self._resize(self.size)

    def insert_at(self, i, x):
        self.insert_last(None)
        self._copy_backward(i, self.size - (i + 1), self.A, i + 1)
        self.A[i] = x

    def delete_at(self, i):
        x = self.A[i]
        self._copy_forward(i + 1, self.size - (i + 1), self.A, i)
        self.delete_last()
        return x

    def insert_first(self, x):
        self.insert_at(0, x)

    def delete_first(self):
        return self.delete_at(0)
```

Excercise 해설 다시 읽어보기.

## Problem Session 1

[Little-O notation](https://en.wikipedia.org/wiki/Big_O_notation#Little-o_notation)

Big-O-notation 관련해서는 교재에 나와있는듯. 수학에서의 쓰임새와 컴퓨터에서의 쓰임새가 다른 것 같다.

Double-Ended Sequence의 구현법에는 양쪽에 빈 공간을 두는 방법과 앞부분용 배열과 뒷부분용 배열을 두는 방법이 있다. 후자의 경우 한 배열이 비게 되면 다시 build한다.

```py
def count_long_subarray(A):
    """
    Input:  A     | Python Tuple of positive integers
    Output: count | number of longest increasing subarrays of A
    """
    temp = []
    subArrs = []
    for i in A:
        if len(temp) == 0:
            temp.append(i)
        elif temp[-1] < i:
            temp.append(i)
        else:
            subArrs.append(temp)
            temp = [i]
    subArrs.append(temp)
    maxLen = max([len(x) for x in subArrs])
    return len([x for x in subArrs if len(x) == maxLen])

# 더 나은 풀이 있으니까 생각해보기
```

## 3. Sets and Sorting

Lecture note은 insertion, selection, merge sort를 재귀적으로 구현하고 각각에 대해 귀납적으로 정당성 증명을 하고 substitution과 recurrence tree를 통한 시간복잡도 증명을 보인다. 좀 지루함,,, 재귀 구현은 좀 장황하고 recitation에 있는 루프를 이용한 구현이 더 깔끔하다.

### 외부 사이트 참조

출처: [How to analyse Complexity of Recurrence Relation](https://www.geeksforgeeks.org/how-to-analyse-complexity-of-recurrence-relation/)

Substitution Method: We make a guess for the solution and then we use mathematical induction to prove the guess is correct or incorrect.

Recurrence Tree Method: In this method, we draw a recurrence tree and calculate the time taken by every level of the tree. Finally, we sum the work done at all levels.

Master Method: Master Method is a direct way to get the solution. The master method works only for the following type of recurrences or for recurrences that can be transformed into the following type.

> T(n) = aT(n/b) + f(n) where a >= 1 and b > 1

[Wikipedia - Mathmatical induction](https://en.wikipedia.org/wiki/Mathematical_induction#Description)

### Set interface

- Container: build
- Static: find
- Dynamic: insert, delete
- Order: find_max, find_min, find_prev(k), find_next(k)

정렬된 배열을 통해 그냥 배열로 구현한 것보다 비교적 효과적인 set을 구현할 수 있다. 정렬을 공부해보자.

원래 배열을 덮어쓰면 **destructive**, O(1)만큼의 추가 공간을 사용하면 **in place**한 정렬 알고리즘이다. destructive는 in place를 포함한다.

(Sorted_Array_Set 구현이 맞나? binary search에서 없는 경우에 대한 처리가 없는 것 같은데)

### Sorting

```py
from itertools import permutations

# 정렬된 배열은 원래 배열의 순열 중 하나임을 보여서
# 끔찍한 알고리즘임에도 기록
# Omega(n!*n)이다. 오메가인 이유는 순열들의 배열을 만드는데 걸리는 시간은 고려 안했기 때문.
def permutation_sort(arr):
    for x in permutations(arr):
        if is_sorted(x):
            return x


def is_sorted(arr):
    for i in range(len(arr) - 1):
        if arr[i + 1] < arr[i]:
            return False
    return True


print(permutation_sort([9, 8, 7, 6, 5, 4, 3, 1]))
```

### Solving Recurrences

```py
# 인덱스 i까지 정렬한다.
def selection_sort(A, i=None):
    if i is None:
        i = len(A) - 1
    if i > 0:
        j = prefix_max(A, i)
        A[i], A[j] = A[j], A[i]
        selection_sort(A, i - 1)


# i까지의 요소들 중 가장 큰 것의 인덱스를 반환한다.
# 기저사례인 i=0일때는 당연히 참이고, i일 때 참이라면 i+1일때의 최댓값은 A[:i+1] 혹은 A[i+1]인데 모든 경우에 잘 반환한다.
# S(1) = theta(1), S(n) = S(n-1) + theta(1).
def prefix_max(A, i):
    if i > 0:
        j = prefix_max(A, i - 1)
        if A[i] < A[j]:
            return j
    return i
```

Insertion sort, Merge sort에서 재귀를 통한 정당성 증명과 substitution, recurrence tree를 통한 시간 복잡도 계산 생략

선택 정렬은 가장 큰 i개의 요소들을 찾으며 쌓아가고, 삽입 정렬은 처음 i개의 요소들을 정렬되게 유지하며 쌓아간다. 둘 다 정렬된 subset들을 키우기에 incremental하다고 한다.

선택 정렬: omega(n^2) comparison, O(n) swaps, 삽입 정렬: omega(n^2) comparison, omega(n^2) swaps.

삽입 정렬은 stable하다.

logn은 n보다 '지수적'으로 느리게 성장한다. 지수적으로 느리게라는 말 어감이 신기.

in-place merge sort도 있다.

### Master Theorem

재귀 호출에서 한 단계 내려갈 때 a배 늘어나고 작업량은 1/b배 감소한다고 하자. f(n)과 n^(logba)를 비교하게 된다. Polynomial하면 간단해진다.

[Master Theorem](<https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)>)

[Akra-Bazzi method](https://en.wikipedia.org/wiki/Akra–Bazzi_method)

## 4. Hashing

### Comparison Model

알고리즘이 요소들을 비교 연산을 통해서만 구분할 수 있다고 가정한다. 비교 연산의 반환값은 True나 False 두가지이다.

알고리즘 수행 시간의 하한은 비교의 횟수에 따라 정해진다.

### Decision Tree

모든 알고리즘은 수행된 작업들의 decision tree로 볼 수 있다.

Comparison model의 경우 내부 노드는 비교 연산을 의미하고 leaf는 알고리즘의 종료, 결과값을 의미한다.

비교 연산의 결과는 binary하므로 가지는 두 개로 갈라진다.

root-to-leaf path는 특정 입력에 대한 알고리즘 실행 추이?를 보여준다.

탐색 알고리즘의 경우 결과값이 없는 경우도 있으니 잎 노드가 최소 n+1개이어야 한다.

**트리의 높이가 알고리즘의 수행 시간을 의미한다.** 따라서 탐색 알고리즘의 경우 logn이 가능한 가장 짧은 트리의 높이이다.

> The relationship between Big Omega (Ω) and Little Omega (ω) is similar to that of Big-Ο and Little o except that now we are looking at the lower bounds. [geeksforgeeks](https://www.geeksforgeeks.org/analysis-of-algorithems-little-o-and-little-omega-notations/)

[Branching factor](https://en.wikipedia.org/wiki/Branching_factor)

To get faster, need an operation that allows super-constant ω(1) branching factor?? 상수 시간을 뛰어넘는 branching factor가 필요하다는 뜻인가.

Most operations within a computer only allow for constant logical branching, like if statements in your code. However, one operation on your computer allows for non-constant branching factor: specifically the ability to randomly access any memory address in constant time.

아무튼 Word-RAM의 O(1) random access를 활용하면 linear branching factor를 얻어낼 수 있다.

레지스터 크기는 보통 w(word)의 크기와 같다.

k = {0, ..., u-1}의 키를 사용하여 요소를 k번째 인덱스에 넣는다.

### Hashing

공간을 너무 많이 차지하므로 해시 함수를 통해 더 작은 direct access array를 사용할 수 있도록 한다.

> Hash function: h(k) : {0, . . . , u − 1} → {0, . . . , m − 1} (also hash map)

Direct access array는 hash table이라 부르고, h(k)는 k라는 키 값의 해시 값이라 한다.

비둘기집 원리에 의해 충돌이 일어날 수밖에 없는데, 다른 곳에 저장하는 open addressing과 dynamic set interface를 제공하는 또다른 자료 구조에 저장하는 chaining이 있다. 전자는 분석이 어렵지만 실용적이고 흔히 사용된다.

Chain size가 theta(n)이면 좋지 않다. 이를 위해 좋은 해시 함수가 필요하다.

### Hashing Functions

> h(k) = (k mod m)

2와 10의 거듭제곱과 거리가 먼 큰 소수가 주로 사용된다.

하지만 입력이 커지면 결국에는 O(n) 크기의 체인을 만들게 된다. If u > nm, every hash function from u to m maps some n keys to the same hash, by the pigeonhole principle.

> For a large enough key domain u, every hash function will be bad for some set of n inputs2. However, we can achieve good **expected** bounds on hash table performance by choosing our hash function **randomly** from a large family of hash functions. Here the expecta- tion is over our choice of hash function, which is independent of the input. **This is not expectation over the domain of possible input keys.**

Deterministic하지 않게 랜덤으로 해시 함수를 고를 수 있게 해보자.

> Universal hash function: h_ab(k) = (((ak + b) mod p) mod m)
>
> Hash Family H(p, m) = {h_ab | a, b ∈ {0, . . . , p − 1} and a != 0}

p는 u보다 큰 고정된 상수값. H는 universal family이다.

Universal family에서 임의로 선택한 해시 함수를 통해 해시한 임의의 두 키의 해시 값이 충돌할 확률은 1/m보다 작거나 같다.

어렵다,,,

위의 h_ab가 universal함은 이 강의에서 다루지 않음. 다만 이를 전제로 평균 체인의 길이가 어떨지는 구할 수 있음.

[Universal hashing](https://en.wikipedia.org/wiki/Universal_hashing)에 정리가 잘 되어 있음.

[Perfect hash function](https://en.wikipedia.org/wiki/Perfect_hash_function)

체이닝으로 충돌을 처리하는 해시 테이블을 universal family에서 임의로 선택한 해시 함수를 통해 구현하면 입력 키들과 무관하게 set 연산들을 **expected constant time**에 처리할 수 있다.

```py
from random import randint
from math import ceil


class Hash_Table_Set:
    def __init__(self, r=200):
        self.chain_set = Set_from_Seq(Linked_List_Seq)
        self.A = []
        self.size = 0
        self.r = r  # fill ratio
        self.p = 2**31 - 1
        self.a = randint(1, self.p - 1)
        self._compute_bounds()
        self._resize(0)

    def __len__(self):
        return self.size

    def __iter__(self):
        for X in self.A:
            yield from X

    def build(self, X):
        for x in X:
            self.insert(x)

    def _hash(self, k, m):
        return ((self.a * k) % self.p) % m

    def _compute_bounds(self):
        self.upper = len(self.A)
        self.lower = len(self.A) * 100 * 100 // (self.r * self.r)

    def _resize(self, n):
        if self.lower >= n or n >= self.upper:
            f = ceil(self.r / 100)
            m = max(n, 1) * f
            A = [self.chain_set() for _ in range(m)]
            for x in self:
                h = self._hash(x.key, m)
                A[h].insert(x)
            self.A = A
            self._compute_bounds()

    def find(self, k):
        h = self._hash(k, len(self.A))
        return self.A[h].find(k)

    def insert(self, x):
        self._resize(self.size + 1)
        h = self._hash(x.key, len(self.A))
        added = self.A[h].insert(x)
        if added:
            self.size += 1
        return added

    def delete(self, k):
        assert len(self) > 0
        h = self._hash(k, len(self.A))
        x = self.A[h].delete(k)
        self.size -= 1
        self._resize(self.size)
        return x


# find_min, find_max, find_next, find_prev, iter_order 생략
```

시간 복잡도에 a(amortized)말고 e(expected?)도 추가됨.

## Problem Session 2

I can tell you that about 82% of my time as a professor is spent with people from industry visiting my office with exactly this kind of scenario where it has a really complicated thing.

They've been thinking about their construction problem for years, and they have all these details and tables and flow charts. And then it turns out that their problem can be captured in about two sentences.

So it's a good skill to have and one that can get you a lot of money as a consultant. So it's one that's worth practicing in this class

4-c, d 풀어보기

## Problem Set 1

Remember that asymptotic growth is much more sensitive to changes in exponents: even if the exponents are both Θ(f(n)) for some function f(n), the functions will not be the same asymptotically unless their exponents only differ by a constant.

However, asymptotically similar logarithms do not imply that the functions are asymptotically the same

```py
class Doubly_Linked_List_Node:
    def __init__(self, x):
        self.item = x
        self.prev = None
        self.next = None

    def later_node(self, i):
        if i == 0:
            return self
        assert self.next
        return self.next.later_node(i - 1)


# 비어있는 경우
# start, end가 각각 head나 tail인 경우
# head, tail을 dummy node로 하고 싶었지만 이미 구현된 메서드를 보니 애매해서 관둠
class Doubly_Linked_List_Seq:
    def __init__(self):
        self.head = None
        self.tail = None

    def __iter__(self):
        node = self.head
        while node:
            yield node.item
            node = node.next

    def __str__(self):
        return "-".join([("(%s)" % x) for x in self])

    def build(self, X):
        for a in X:
            self.insert_last(a)

    def get_at(self, i):
        node = self.head.later_node(i)
        return node.item

    def set_at(self, i, x):
        node = self.head.later_node(i)
        node.item = x

    def insert_first(self, x):
        ###########################
        # Part (a): Implement me! #
        ###########################
        if self.head is None:
            self.head = Doubly_Linked_List_Node(x)
            self.tail = self.head
        else:
            node = Doubly_Linked_List_Node(x)
            node.next = self.head
            self.head.prev = node
            self.head = node

    def insert_last(self, x):
        ###########################
        # Part (a): Implement me! #
        ###########################
        if self.tail is None:
            self.insert_first(x)
        else:
            node = Doubly_Linked_List_Node(x)
            self.tail.next = node
            node.prev = self.tail
            self.tail = node

    def delete_first(self):
        x = None
        ###########################
        # Part (a): Implement me! #
        ###########################
        x = self.head
        if self.head is None:
            return x
        if self.head.next is None:
            self.head = None
            self.tail = None
        else:
            self.head.next.prev = None
            self.head = self.head.next
        return x.item

    def delete_last(self):
        x = None
        ###########################
        # Part (a): Implement me! #
        ###########################
        x = self.tail
        if self.tail is None:
            return x
        if self.tail.prev is None:
            self.head = None
            self.tail = None
        else:
            self.tail.prev.next = None
            self.tail = self.tail.prev
        return x.item

    def remove(self, x1, x2):
        L2 = Doubly_Linked_List_Seq()
        ###########################
        # Part (b): Implement me! #
        ###########################
        L2.head = x1
        L2.tail = x2

        if x1 is self.head:
            self.head = x2.next
        else:
            x1.prev.next = x2.next

        if x2 is self.tail:
            self.tail = x1.prev
        else:
            x2.next.prev = x1.prev

        x1.prev = None
        x2.next = None

        return L2

    def splice(self, x, L2):
        ###########################
        # Part (c): Implement me! #
        ###########################
        if x.next is None:
            x.next = L2.head
            L2.head.prev = x
            self.tail = L2.tail
        else:
            x.next.prev = L2.tail
            L2.tail.next = x.next
            x.next = L2.head
            L2.head.prev = x
        L2.head = None
        L2.tail = None
```

## 5. Linear Sorting

지난 problem에서 Worst case O(n)이면 해시 테이블을 사용해서는 안된다.

Subscript e는 expected runtime을 뜻한다. A는 amortized runtime.

### 비교 기반 정렬의 하한

Comparison model에 따라 알고리즘 결정 트리는 binary이고, 잎 노드는 순열의 개수인 n!개이다.

따라서 트리 높이의 하한은 log(n!) ≥ log((n/2)n/2) = Ω(n log n)이다. 스털링 근사로 증명할 수도 있다.

### Direct Access Array Sort

Running time is Θ(u + n), which is Θ(n) if u = Θ(n). 배열 만들고 순회하는데 u, 요소 삽입하는데 n이 소요된다.

그런데 n크기의 배열을 만드는데 O(n)이 필요한게 맞나?

```py
def direct_access_sort(A):
    u = 1 + max([x.key for x in A])
    D = [None] * u
    for x in A:
        D[x.key] = x
    i = 0
    for key in range(u):
        if D[key] is not None:
            A[i] = D[key]
            i += 1
```

하지만 중복되는 키가 있거나 키 값의 범위가 u = Ω(n2) < n2같으면 좋지 못하다. 해결책은 counting sort를 살펴보고 보자.

### Counting Sort

Can we modify direct access array sort to admit multiple keys in a way that is stable? 에 대한 대답으로 counting sort를 소개한다. Insertion order를 보존하고 **stable**하다.

O(n + u) 시간이 소요된다. Array sort와 마찬가지고 u = O(n)이면 리니어하지만 이번에는 **중복된 키를 허용**한다.

```py
def counting_sort(A):
    u = 1 + max([x.key for x in A])
    D = [[] for _ in range(u)]
    for x in A:
        D[x.key].append(x)
    i = 0
    for chain in D:
        for x in chain:
            A[i] = x
            i += 1


# 최종 인덱스 위치를 cumulative sum을 통해 계산한다.
# [5, 3, 1, 4, 1']
def counting_sort2(A):
    # u = 6
    u = 1 + max([x.key for x in A])
    # D = [0, 0, 0, 0, 0, 0]
    D = [0] * u
    for x in A:
        D[x.key] += 1
    # D = [0, 2, 0, 1, 1, 1]
    for k in range(1, u):
        D[k] += D[k - 1]
    # D = [0, 2, 2, 3, 4, 5]
    for x in list(reversed(A)):
        A[D[x.key] - 1] = x
        D[x.key] -= 1
```

### Tuple Sort

키로부터 튜플이 만들어지는데, 이를 정렬하기 위해 중요도가 낮은 것부터 정렬한다. 이때 이전에 정렬한 정보가 사라지면 곤란하니 counting sort와 같은 stable한 sort 알고리즘을 사용해야한다.

### Radix Sort

각각의 키를 적당히 쪼개 몫과 나머지를 활용한다. Multiples of powers of n으로 쪼개면 base n으로 표현했을 때의 digit이 된다. c자리의 수를 요소가 c개인 튜플로 표현할 수 있다.

Tuple sort와 counting sort의 이러한 조합을 radix sort라 한다.

If the integers are non-negative and the largest integer in the set is u, then this base n number will have ceil(log_n_u) digits. If the largest integer in the set u ≤ nc, then radix sort runs in O(nc) time. Thus, if c is constant, then radix sort also runs in linear time!

```py
from counting_sort import counting_sort


def radix_sort(A):
    n = len(A)
    u = 1 + max([x.key for x in A])
    # 왜??
    # ceil을 쓰면 안되나? u와 n의 자릿수가 같으면 어떻게 되지.
    # divmod에서 n으로 나누고, 자릿수가 같아도 u가 더 크면 결국에 n으로 한번 나누어줘야한다.
    # 그래서 +1을 해주는듯. u가 더 작으면 radix sort를 쓸 이유가 없긴 하지만 비효율적으로 돌아갈듯?
    c = 1 + (u.bit_length() // n.bit_length())

    class Obj:
        pass

    D = [Obj() for _ in A]
    for i in range(n):
        D[i].digits = []
        D[i].item = A[i]
        high = A[i].key
        for _ in range(c):
            high, low = divmod(high, n)
            D[i].digits.append(low)
    for i in range(c):
        for j in range(n):
            D[j].key = D[j].digits[i]
        counting_sort(D)
    for i in range(n):
        A[i] = D[i].item
```

## Problem Session 3

### 3-2

정수를 0 이상으로 매핑할 수 있다. 양수는 2를 곱하고, 음수는 -2를 곱하고 1을 뺀다. 이는 정수와 양수가 같은 크기의 무한임을? 보일때도 사용하는 테크닉인데 이를 통해 별도의 배열을 관리할 필요 없이 insert_first를 구현할 수 있다.

Invariant: keys = { first, first+1, ... , first + n - 1}

### 3-3

Radix sort on base 26 numbers.

tuple sort면 O(nlogn), radix sort면 base 26이 아니라 base n을 사용하므로 linear time으로 된다?? 전자는 문자마다하고 그리고,후자는 한번에 logn개를 해서 더 빠르다.

two-finger-algorithm.

sliding window technique.

## Problem Set 2

pass

## 6. Binary Trees I

> 트리의 정의(위키피디아): Each node in the tree can be connected to many children (depending on the type of tree), but must be connected to exactly one parent, except for the root node, which has no parent.

강의에서는 트리를 사이클이 없는 그래프(a connected graph with no cycles)로 정의한다.

Binary tree는 노드 당 세 개의 포인터를 가지는 포인터 기반 자료구조이다. 부모 노드도 가르킴이 신기하네,,,

### Terminology

트리 자체의 깊이는 정의되지 않는다. 깊이는 노드만 가진다. 트리의 높이는 존재하는 듯.

루트의 높이 h에 대해서 O(h)의 operation들을 정의하고, 이후 h = O(logn)이 되도록 유지시켜보자.

강의에서 tree traversal은 inorder traversal을 의미함.

Binary tree는 binary node들로 구성된 트리이다. 상수 개수의 field들을 가진다. 노드에 저장된 아이템으로의 포인터, 부모 노드로의 포인터, 왼쪽 자식, 오른쪽 자식으로의 포인터. 뒤에 세 개는 None일 수 있다.

### Tree Navigation

find first/last는 각각 맨 왼쪽, 오른쪽을 찾으면 된다. 각각은 symmetrical하다.

find successor: 오른쪽 노드가 있으면 오른쪽 서브 트리의 첫번째 노드. 없으면 x를 왼쪽 서브 트리에 가지는 가장 낮은(lowest, 정의가 뭘까... height의 맥락에서 높고 낮음을 이야기하는듯?) 선조 노드를 반환한다.

### Dynamic Operations

find predecessor: 왼쪽 노드가 있으면 왼쪽 서브 트리의 첫번째 노드를 반환, 없으면 x를 오른쪽 서브 트리로 가지는 가장 낮은 선조 노드를 반환.

insert after/before도 대칭적이다.

insert after: 오른쪽 자식 없으면 거기 넣음. 있으면 기준 노드의 successor의 왼쪽 노드에 넣는다. Successor는 왼쪽 하단 노드니 왼쪽 자식이 있을 수가 없다.

insert before: 왼쪽 자식 없으면 거기 넣음. 있으면 기준 노드의 predecessor의 오른쪽 노드에 넣는다.

delete: leaf면 parent에서 떼서 반환. 왼쪽 자식이 있으면 대상 노드의 predecessor와 대치시키고 잎으로 갈 때까지 재귀 반복한다. 오른쪽 자식이 있으면 대상 노드의 successor와 대치시키고 재귀 반복한다.

### Application

트리의 순회 순서와 저장된 요소들간에 sementic connection을 주어 set 혹은 sequence로 활용할 수 있다.

Set: Set Binary Tree(a.k.a Binary Search Tree, BST). 순회 순서가 키 값의 순서와 같게 한다. 왼쪽 서브 트리의 키 값 <= 노드의 키 값 <= 오른쪽 서브 트리의 키 값.

Sequence: 순회 순서가 시퀀스의 순서. 서브 트리의 크기만 알 수 있다면 O(h)의 시간에 인덱스 접근을 할 수 있다. Augmentation을 통해 서브 트리의 크기를 알아낸다. leaf가 추가될 때마다 모든 선조 노드의 size에 1을 더한다. 삭제할 때도 마찬가지. 나이브한 구현으로는 build(X)가 O(nh)가 소요된다.

Lecture note 마지막에 build 시간 복잡도가 왜이렇게 좋지?

```py
class Binary_Node:
    def __init__(A, x):
        A.item = x
        A.left = None
        A.right = None
        A.parent = None
        # A.subtree_update()

    # self 대신에 A로 쓸 수도 있네
    # 여기서는 __iter__를 안쓰네?
    def subtree_iter(A):
        if A.left:
            yield from A.left.subtree_iter()
        yield A
        if A.right:
            yield from A.right.subtree_iter()

    def subtree_first(A):
        if A.left:
            return A.left.subtree_first()
        else:
            return A

    def subtree_last(A):
        if A.right:
            return A.right.subtree_last()
        else:
            return A.right

    # 없는 경우는 None이 반환되는 듯.
    def successor(A):
        if A.right:
            return A.right.subtree_first()
        while A.parent and (A is A.parent.right):
            A = A.parent
        return A.parent

    def predecessor(A):
        if A.left:
            return A.left.subtree_last()
        while A.parent and (A is A.parent.left):
            A = A.parent
        return A.parent

    def subtree_insert_before(A, B):
        if A.left:
            A = A.left.subtree_last()
            # parent도 관리해줘야함 주의
            A.right, B.parent = B, A
        else:
            A.left, B.parent = B, A
        # A.maintain()

    def subtree_insert_after(A, B):
        if A.right:
            A = A.right.subtree_first()
            A.left, B.parent = B, A
        else:
            A.right, B.parent = B, A
        # A.maintain()

    def subtree_delete(A):
        if A.left or A.right:
            if A.left:
                B = A.predecessor()
            else:
                B = A.successor()
            A.item, B.item = B.item, A.item
            return B.subtree_delete()
        if A.parent:
            if A.parent.left is A:
                A.parent.left = None
            if A.parent.right is A:
                A.parent.right = None
            # A.parent.maintain()
        return A


class Binary_Tree:
    def __init__(T, Node_Type=Binary_Node):
        T.root = None
        T.size = 0
        T.Node_Type = Node_Type

    def __len__(T):
        return T.size

    def __iter__(T):
        if T.root:
            for A in T.root.subtree_iter():
                yield A.item


# Binary tree로 Set interface를 구현하려면 키 순서가 순회 순서가 되도록 키를 저장한다. 왼쪽 서브트리의 키들이 작고 오른쪽 서브트리의 키들이 큰 특징을 Binary Search Tree Property라 부른다.


class BST_Node(Binary_Node):
    def subtree_find(A, k):
        if k < A.item.key:
            if A.left:
                return A.left.subtree_find(k)
        elif k > A.item.key:
            if A.right:
                return A.right.subtree_find(k)
        else:
            return A
        return None

    def subtree_find_next(A, k):
        if A.item.key <= k:
            if A.right:
                return A.right.subtree_find_next(k)
            else:
                return None
        elif A.left:
            B = A.left.subtree_find_next(k)
            if B:
                return B
        return A

    def subtree_find_prev(A, k):
        if k <= A.item.key:
            if A.left:
                return A.left.subtree_find_prev(k)
            else:
                return None
        elif A.right:
            B = A.right.subtree_find_prev(k)
            if B:
                return B
        return A

    def subtree_insert(A, B):
        if B.item.key < A.item.key:
            if A.left:
                # 이건 여기 클래스에서 정의한거
                A.left.subtree_insert(B)
            else:
                # 이건 아까 정의한거
                A.subtree_insert_before(B)
        elif B.item.key > A.item.key:
            if A.right:
                A.right.subtree_insert(B)
            else:
                A.subtree_insert_after(B)
        else:
            A.item = B.item


class Set_Binary_Tree(Binary_Tree):
    def __init__(self):
        # 오 이렇게 하네?
        super().__init__(BST_Node)

    def iter_order(self):
        yield from self

    def build(self, X):
        for x in X:
            self.insert(x)

    def find_min(self):
        if self.root:
            return self.root.subtree_first().item

    def find_max(self):
        if self.root:
            return self.root.subtree_last().item

    def find(self, k):
        if self.root:
            node = self.root.subtree_find(k)
            if node:
                return node.item

    def find_next(self, k):
        if self.root:
            node = self.root.subtree_find_next(k)
            if node:
                return node.item

    def find_prev(self, k):
        if self.root:
            node = self.root.subtree_find_prev(k)
            if node:
                return node.item

    def insert(self, x):
        new_node = self.Node_Type(x)
        if self.root:
            self.root.subtree_insert(new_node)
            # 이미 있는 키인 경우
            if new_node.parent is None:
                return False
        else:
            self.root = new_node
        self.size += 1
        return True

    def delete(self, k):
        assert self.root
        node = self.root.subtree_find(k)
        assert node
        ext = node.subtree_delete()
        if ext.parent is None:
            self.root = None
        self.size -= 1
        return ext.item
```

## 7. Binary Trees II: AVL

### Balanced Binary Trees

Dynamic operation 후에도 O(logn)의 높이를 유지하는 이진 트리를 균형잡혔다(balanced)고 한다. 첫번째로 제안된게 AVL 트리.

### Rotations

순회 순서는 유지한채 트리를 균형잡히게 하기 위해 rotation을 사용한다.

> A rotation takes a subtree that locally looks like one the following two configurations(?) and modifies the connections between nodes in O(1) time to transform it into the other configuration.

[Is it always possible to turn one BST into another using tree rotations?](https://stackoverflow.com/questions/14027726/is-it-always-possible-to-turn-one-bst-into-another-using-tree-rotations)

명제: O(n)번의 rotation으로 같은 순회 순서인 모든 트리간의 변환이 가능하다.

증명: 순회 순서에서 가능한 뒤쪽 노드부터 right rotation을 반복(last possible right rotation)하면 어떤 트리든 canonical chain으로 환원된다. 하나는 체인으로, 하나는 체인까지의 역방향으로 하고 이으면 변환 가능. 각 rotation은 마지막 노드의 깊이를 1씩 늘리므로 최대 n-1의 rotation, O(n)이 소요된다.

증명을 이해는 했는데 뭔가 건너뛴 느낌?

어쨌든 모든 binar tree는 O(n)의 rotation으로 균형을 잡을 수 있다. 느리니까 다른 방법도 알아보자.

```py
def subtree_rotate_right(D):
    assert D.left
    B, E = D.left, D.right
    A, C = B.left, B.right
    # 그림에서 위치를 바꾼다고 생각하기
    D, B = B, D
    D.item, B.item = B.item, D.item
    # 이후 모든 연결을 다시 만든다고 생각.
    # 그림에서 연속적으로 바뀐다고 생각하면 더 헷갈린다.
    B.left, B.right = A, D
    D.left, D.right = C, E
    if A:
        A.parent = B
    if E:
        E.parent = D
    # 아래 두 노드는 높이가 바뀐다. 
    B.subtree_update()
    D.subtree_update()


def subtree_rotate_left(B):
    assert B.right
    A, D = B.left, B.right
    C, E = D.left, D.right
    B, D = D, B
    D.left, D.right = B, E
    B.left, B.right = A, C
    if A:
        A.parent = B
    if E:
        E.parent = D
    B.subtree_update()
    D.subtree_update()
```

### Skew

> **Skew**: The height of node's right subtree minus that of node's left subtree

Height-balanced tree는 모든 노드의 skew값이 -1, 0, 1이다.

명제: Height-balanced binary tree는 높이 h = O(logn)이다.

증명: h = O(logn)는 logn의 하한이 omega(h)이라는 것이므로 n = 2^omega(h)와 동치이다. 따라서 height-balanced tree의 최소 노드의 개수가 적어도 h에 지수적임을 증명하면 된다. Let F (h) denote the fewest nodes in any height-balanced tree of height h. n은 노드의 개수. 높이 h인 트리에 대해 최소 노드의 개수를 F(h)라 하면 F(h) = 1 + F(h-1) + F(h-2) >= 2F(h-2). 본인 + skew의 조건을 만족시키는 선에서 최소 자식 노드들의 개수.

### Maintaining Height-Balance

```py
def skew(A):
    return height(A.right) - height(A.left)


def rebalance(A):
    if A.skew() == 2:
        if A.right.skew() < 0:
            A.right.subtree_rotate_right()
        A.subtree_rotate_left()
    elif A.skew() == -2:
        if A.left.skew() > 0:
            A.right.subtree_rotate_left()
        A.subtree_rotate_right()


def maintain(A):
    A.rebalance()
    A.subtree_update()
    if A.parent:
        A.parent.maintain()


# O(1) height
def height(A):
    if A:
        return A.height
    else:
        return -1

# insertion, deleteion, rotation에서 불린다.
def subtree_update(A):
    A.height = 1 + max(height(A.left), height(A.right))
```

Dynamic operation으로 트리의 높이와 skew가 바뀌면 잎부터 루트까지 height-balance를 고친다. 삭제나 삽입은 결과적으로 leaf node에서 이루어진다.

B의 skew가 2일 때 B에서 left rotation. 만약 F가 -1이면 F에서 right 이후 B에서 left. 자세한건 PDF 참조.

Case 3에서 A의 높이가 h일 때 G의 높이도 h임을 이해하기.

명제: 잎 노드를 추가하거나 삭제하여 트리 T -> T'로 갔을 떄 T'는 O(logn)의 rotation 후에 균형 잡힌 트리가 될 수 있다.

증명: 노드가 삽입되었을 때는 Local Rebalancing Case 2 or 3 후 다른 작업이 필요하지 않다. 노드가 삭제되었을 때는 rebalance 후 높이가 줄어들 수 있기에 조상 노드들에 총 O(logn)의 작업을 해야한다. 아우 어려워,,,

### Computing Height

이전 강의에 나온 tree를 이용한 sequence에는 size가 필요하다.

Binary Tree Augmentation: 하위 트리랑만 관련된 값들(size, height...)은 미리 계산해놓을 수 있다. 업데이트도 O(h)시간에 가능하다. 깊이, 인덱스는 트리 전체를 사용하는? 정보이기에 불가능하다.

> the idea behind augmentation is to store additional information at each node so that information can be queried quickly in the future.

**Subtree property** P로 이진 트리를 augment하려면 P를 각 노드에 저장해야하고 노드의 children으로부터 P를 상수 시간내에 계산할 수 있어야 한다. 이러면 P는 dynamic operation 비용을 바꾸지 않고 잘 관리될 수 있다.

Relinked 노드는 조상 노드가 바뀌지 않았으니 O(1) 시간에 업데이트할 수 있고, 노드의 삭제나 삽입의 경우 O(h) 시간동안 조상 노드들을 업데이트해야한다.

### Applications

모든 set 자료 구조는 정렬 알고리즘은 내포한다 - build(or 반복된 insert) 후 iter. AVL sort는 새로운 O(nlgn) 알고리즘이다.

트리로 시퀀스를 구현하려면 subtree property로 size가 꼭 필요하다.

```py
# skew 함수에서 무지성 호출하기때문에 None일 때의 처리가 필요하다.
def height(A):
    if A:
        return A.height
    else:
        # 0이 아니다
        return -1


class Binary_Node:
    def __init__(A, x):
        A.item = x
        A.left = None
        A.right = None
        A.parent = None
        A.subtree_update()

    # 함수명이 좀,,,
    def subtree_update(A):
        A.height = 1 + max(height(A.left), height(A.right))

    def skew(A):
        return height(A.right) - height(A.left)

    def subtree_iter(A):
        if A.left:
            yield from A.left.subtree_iter()
        yield A
        if A.right:
            yield from A.right.subtree_iter()

    def subtree_first(A):
        if A.left:
            return A.left.subtree_first()
        else:
            return A

    def subtree_last(A):
        if A.right:
            return A.right.subtree_last()
        else:
            return A

    def sucessor(A):
        if A.right:
            return A.right.subtree_first()
        while A.parent and A is A.parent.right:
            A = A.parent
        return A.parent

    def predecessor(A):
        if A.left:
            return A.left.subtree_last()
        # ==와 is의  차이
        # https://stackoverflow.com/questions/132988/is-there-a-difference-between-and-is
        while A.parent and A is A.parent.left:
            A = A.parent
        return A.parent

    # 냅다 predecessor를 찾으면 left가 이미 있을수도 있다.
    # 아니다 그것보다는 predecessor를 찾아서 한다는 논리가 이상함.
    # 아래 코드에 predecessor를 찾을 때 사용한 로직이 있기는 하다(왼쪽 노드 있을 때)
    # 다만 왼쪽 노드가 없으면 predecessor를 찾을 필요도 없고 냅다 거기 넣으면 됨.
    def subtree_insert_before(A, B):
        if A.left:
            A = A.left.subtree_last() 
            # A.left가 아니다!!
            A.right, B.parent = B, A
        else:
            A.left, B.parent = B, A
        A.maintain()

    def subtree_insert_after(A, B):
        if A.right:
            A = A.right.subtree_first()
            A.left, B.parent = B, A
        else:
            A.right, B.parent = B, A
        A.maintain()

    def subtree_delete(A):
        if A.left or A.right:
            if A.left:
                # if문 안에서 선언했는데 살아남네
                B = A.predecessor()
            else:
                B = A.successor()
            A.item, B.item = B.item, A.item
            return B.subtree_delete()
        if A.parent:
            if A.parent.left is A:
                A.parent.left = None
            else:
                A.parent.right = None
            A.parent.maintain()
        return A

    # D가 중심?이니 self 대신 D로 명명
    def subtree_rotate_right(D):
        assert D.left
        B, E = D.left, D.right
        A, C = B.left, B.right
        B, D = D, B
        B.item, D.item = D.item, B.item
        B.left, B.right = A, D
        D.left, D.right = C, E
        # C의 parent는 이미 D이기에 상관 없음
        # 그러면 D.left = C가 필요한가? 오른쪽이었다가 왼쪽 가는거라서 필요함
        # 식별자와 객체 관계 개념 헷갈리니까 정신 나감
        if A:
            A.parent = B
        if E:
            E.parent = D
        B.subtree_update()
        D.subtree_update()

    # B가 중심?이니 self 대신 D로 명명
    def subtree_rotate_left(B):
        assert B.right
        A, D = B.left, B.right
        C, E = D.left, D.right
        B, D = D, B
        B.item, D.item = D.item, B.item
        B.left, B.right = A, C
        D.left, D.right = B, E
        # rotate_left와 right 모두 C는 업데이트할 필요가 없다.
        # 머리로 애니메이션해보면 C만 부모 노드가 바뀌지만,
        # 여기 구현에서는 B와 D를 바꿔치기하고 연결하기 때문에 부모 노드가 그대로인 셈이다.
        if A:
            A.parent = B
        if E:
            E.parent = D
        B.subtree_update()
        D.subtree_update()

    def rebalance(A):
        if A.skew() == 2:
            if A.right.skew() < 0:
                A.right.subtree_rotate_right()
            A.subtree_rotate_left()
        elif A.skew() == -2:
            if A.left.skew() > 0:
                A.left.subtree_rotate_left()
            A.subtree_rotate_right()

    # insert_before/after, delete 이후 불린다.
    # delete는 한번만 해도 된다는 것 같은데 parent를 무조건 호출하네,,,?
    def maintain(A):
        A.rebalance()
        # 자식 노드들에 대해서는 업데이트 안해줘도 되나?
        # 아 rotate에서 해준다.
        # 그러면 이거는 중복 아닌가?
        A.subtree_update()
        if A.parent:
            A.parent.maintain()
```

R06에서 사용한 방법을 통해 O(n) 시간에 균형잡힌 이진 트리를 만들어낼 수 있다. 이렇게 만들어진 자료 구조를 **Sequence AVL**이라 한다.

```py
import Binary_Node


class Size_Node(Binary_Node):
    def subtree_update(A):
        super().subtree_update()
        A.size = 1
        if A.left:
            A.size += A.left.size
        if A.right:
            A.size += A.right.size

    # i == 0이라고 A를 반환하면 안됨!!!
    def subtree_at(A, i):
        assert 0 <= A
        if A.left:
            L_size = A.left.size
        else:
            L_size = 0
        # L_size = A.left.size if A.left else 0
        if i < L_size:
            return A.left.subtree_at(i)
        elif i > L_size:
            return A.right.subtree_at(i - L_size - 1)
        else:
            return A
```

Set_Binary_Tree는 생략

Excercise T.build 결과가 왜 나랑 다르지??

그나저나 inorder traversal을 하기 때문에 이런 augmentation이 가능한 듯. 안그러면 부모의 값을 알아야하는데 이상해진다.

## Problem Session 4

Priority queue 배우고 돌아오기

## Problem Set 3

```py
A = [47, 61, 36, 52, 56, 33, 92]
print("\t".join([str((10 * k + 4) % 7) for k in A]))

print()

for c in range(7, 100):
    hashes = [((10 * k + 4) % c) % 7 for k in A]
    print("\t".join([str(h) for h in hashes]))
    if len(set(hashes)) == 7:
        break
```

모듈러 연산 관련해서 공부하기.

## 8. Binary Heaps

### 우선순위 큐 인터페이스

> Keep track of many items, quickly access/remove the most important

build(X), insert(x), delete_max(), find_max(). build와 find_max는 insert와 delete_max로 구현할 수 있으니 insert와 delete_max에 집중해보자.

### 우선순위 큐 정렬

> Priority queues provide a general framework for at least three sorting algorithms, which differ only in the data structure used in the implementation.

우선순위 큐 자료 구조는 정렬 알고리즘으로 변환할 수 있다. 다 넣고 하나씩 빼면 정렬된 결과가 나온다.

T_build + n \* T_delete_max <= n \* T_insert + n \* T_delete_max

이를 dynamic array에 적용하면 selection sort, sorted dynamic array에 적용하면 insertion sort가 된다.

AVL Tree로 우선순위 큐를 구현할 때 subtree augmentation으로 find_min과 find_max를 O(1)에 수행 가능하다.

AVL Tree를 통한 정렬과 다르게 heap을 사용하면 inplace로 가능하다.

### Binary Heaps

배열과 완전 이진 트리간의 일대일 대응이 가능하다. 자식과 부모 노드 구하는 연산은 생략.

implicit data structure: no pointers, just an array of n items.

Max-Heap Property at node i: Q[i] ≥ Q[j] for j ∈ {left(i), right(i)}. 최대 힙은 모든 노드에서 이 성질을 만족시킨다.

• Claim: In a max-heap, every node i satisfies Q[i] ≥ Q[j] for all nodes j in subtree(i)

• Proof:

- Induction on d = depth(j) − depth(i)
- Base case: d = 0 implies i = j implies Q[i] ≥ Q[j] (in fact, equal)
- depth(parent(j)) − depth(i) = d − 1 < d, so Q[i] ≥ Q[parent(j)] by induction – Q[parent(j)] ≥ Q[j] by Max-Heap Property at parent(j)

[Why isn't heapsort stable?](https://stackoverflow.com/questions/19336881/why-isnt-heapsort-stable)

### In-place Priority Queue Sort

In-place priority queue sort with Array is exactly Selection Sort, Sorted Array는 Insertion Sort, binary Max Heap은 Heap Sort

max_heapify_up이 아닌 max_heapify_down으로 build하면 O(n)으로 가능하다.

Sequence AVL Tree에서도 O(n)으로 build가 가능했다. 최대값을 가진 노드의 포인터를 augmentation으로 저장하면 우선순위 큐를 구현할 수 있다.

Multiset을 위해 sequence에 같은 키인 요소들을 저장해야되는 다른 자료 구조들과 다르게, binary heap과 AVL tree는 크기 비교할 때 <=를 <대신 사용하기만 한다면 multiset을 구현할 수 있다.

### 코드

```py
from InPlacePQ import PriorityQueue


class PQ_Array(PriorityQueue):
    # insert는 기본 구현이 유효하다.

    def delete_max(self):
        n, A, m = len(self.A), self.A, 0
        for i in range(1, len(self.A)):
            if A[m].key < A[i].key:
                m = i
        A[m], A[n] = A[n], A[m]
        return super().delete_max()


class PQ_SortedArray(PriorityQueue):
    # insert는 기본 구현이 유효하다.

    # 0개도 허용하지 위함
    # 튜플로 들어가는데???
    def insert(self, *args):
        super().insert(*args)
        i, A = len(self.A) - 1, self.A
        while 0 < i and A[i + 1].key < A[i].key:
            A[i + 1], A[i] = A[i], A[i + 1]
            i -= 1


class PQ_Heap(PriorityQueue):
    def insert(self, *args):
        super().insert(*args)
        n, A = self.n, self.A
        max_heapify_up(A, n, n - 1)

    # 그냥 PriorityQueue를 상속?받을 때와 InPlacePQ를 상속받을 때와 구현이 다르다.
    # 아래는 후자에 맞춰진 상태
    # 아니가 그냥 뭔가 이상하게 섞여있음.
    # InPlacePQ인 경우는 len 관련된거를 다 지워야할듯
    def delete_max(self):
        n, A = self.n, self.A
        A[0], A[n - 1] = A[n - 1], A[0]
        max_heapify_down(A, n - 1, 0)
        return super().delete_max()


# 에러를 던지는 대신 항상 유효한 인덱스 값을 반환하는 것은 곧 짤 코드를 간략화시키는데 도움이 된다.
# 클래스 안에서 선언하니 식별자를 찾지를 못한다. self.어쩌구로 해야되서 그런듯
def parent(i):
    p = (i - 1) // 2
    return p if 0 < i else i


def left(i, n):
    l = 2 * i + 1
    return l if l < n else i


def right(i, n):
    r = 2 * i + 2
    return r if r < n else i


def max_heapify_up(A, n, c):
    p = parent(c)
    # p == c인 경우는 여기서 걸린다.
    # 그런데 이러면 같은 키 값에 대한 처리를.. 할 수 있네
    if A[p].key < A[c].key:
        A[c], A[p] = A[p], A[c]
        max_heapify_up(A, n, p)


def max_heapify_down(A, n, p):
    l, r = left(p, n), right(p, n)
    c = l if A[r].key < A[l].key else r
    if A[p].key < A[c].key:
        A[c], A[p] = A[p], A[c]
        max_heapify_down(A, n, c)


class Sample:
    def __init__(self, key):
        self.key = key


A = [7, 3, 5, 6, 2, 0, 3, 1, 9, 4]
AA = [Sample(x) for x in A]
print([x.key for x in PQ_Heap.sort(AA)])
```

전체 배열에 접근 가능할 때, 잎에서 루트 방향으로 힙을 만듦으로서 O(n)시간에 힙을 만들 수 있다.

```py
def build_max_heap(A):
    n = len(A)
    # 잎 부분은 heapify_down이 의미가 없다.
    # 내려갈 곳이 없음.
    for i in range(n // 2, -1, -1):
        max_heapify_down(A, n, i)
```

아래와 같이 기본 구현을 바꾸어 PQ_Array, PQ_SortedArray, PQ_Heap을 in-place로 바꿀 수 있다. len(self.A) 썼던거 안바꿔도 되나??

> The insert function is no longer given a value to insert; instead, it inserts the item already stored in A[n], and incorporates it into the now-larger queue.

> Similarly, delete max does not return a value; it merely deposits its output into A[n] before decreasing its size.

```py
class PriorityQueue:
    def __init__(self, A):
        self.n, self.A = 0, A

    def insert(self):
        if not self.n < len(self.A):
            raise IndexError('insert into full priority queue')
        self.n += 1

    def delete_max(self):
        if self.n < 1:
            raise IndexError('pop from empty priority queue')
        self.n -= 1 # NOT correct on its own!

    @classmethod
    def sort(Queue, A):
        pq = Queue(A)
        for i in range(len(A)):
            pq.insert()
        for i in range(len(A)):
            pq.delete_max()
        return pq.A
```

Proximate Sorting 뭐냐 겁나 어려움 pass ㅎ

### 기타 메모

- [ADT에 complexity가 포함되어야하는지](https://en.wikipedia.org/wiki/Abstract_data_type). 이건 갑자기 궁금해서 ㅎ.
- 배열의 요소를 삭제하는 방법에는 마지막 요소와 swap한 뒤 마지막 요소를 없애는 방법도 있다.
- index arithmetic.
- percolate up/down
- [Floor and ceiling functions](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions)
- [Priority stack](https://stackoverflow.com/questions/55207148/priority-stack-in-c)

## 9. Breadth-First Search

지금까지는 자료구조와 정렬, 챕터 9부터는 그래프 알고리즘에 대하여 다룬다.

모든 네트워크 시스템은 그래프와 연관되어있다. Discrete system의 상태 공간은 전이 그래프로 표현할 수 있다. 체스, 테트리스, 루미큐브 등등...

### Graph Definitions

> Graph G = (V, E) is a set of vertices V and a set of pairs of vertices E ⊆ V × V.

(u, v)면 directed graph, {u, v}면 undirected graph.

강의에서 다루는 그래프는 simple하고, 따라서 |E| = O(|V|^2)이다.

outgoing neighbor set(Adj+), incoming neightbor set(Adj-), out-degree, in-degree. 위 첨자가 없으면 outgoing으로 이해하자.

> A graph is called **strongly connected** if there is a path from every node to every other node in the graph.

### Graph Representations

u와 Adj(u)를 매핑하는 Adj는 set 자료 구조를 사용한다. Lookup이 빠르면 좋으니 보통 direct access array/hash table을 사용한다.

Adj(u)의 값은 adjacency list라는 자료 구조를 사용한다. Iteration이 주로 사용되므로 array나 linked list를 사용한다. 이 방식은 특정 엣지가 존재하는걸 알려면 Omega(|V|)가 필요하다. 이런 연산이 자주 필요하면 해시 테이블을 사용하면 되지만, 강의에서는 iteration이 주라서 간단하게 unsorted-array-based adjacency list representation을 사용한다.

일반적인 표현법으로는 Adj는 Theta(|V|), Adj(u)는 Theta(deg(u))의 공간을 사용한다.

[Handshaking lemma](https://en.wikipedia.org/wiki/Handshaking_lemma)에 의해 V에 속한 u에 대해 deg(u) <= 2|E|이므로 그래프는 Theta(|V|+|E|)의 공간에 저장 가능하다. 따라서 그래프 알고리즘의 맥락에서 리니어하다는건 Theta(|V|+|E|)임을 의미한다.

### Path

> A path is a sequence of vertices p = (v1,v2,...,vk) where (vi,vi+1) ∈ E for all 1 ≤ i < k.

Vertex를 반복하지 않으면 경로는 simple하다고 한다. 이 강의에서는 얘만 다룸.

거리(distance)는 최소 길이를 가진 경로의 길이를 의미한다.

SINGLE PAIR REACHABILITY(G, s, t): is there a path in G from s ∈ V to t ∈ V ?

SINGLE PAIR SHORTEST PATH(G, s, t): return distance δ(s, t), and a shortest path in G = (V, E) from s ∈ V to t ∈ V

SINGLE SOURCE SHORTEST PATHS(G, s): return δ(s, v) for all v ∈ V , and a shortest-path tree containing a shortest path from s to every v ∈ V. SSSP

뒷 순서의 문제를 풀면 위 순서의 문제도 풀 수 있다. 제일 어려운 문제를 O(|V| + |E|)시간에 푸는 방법을 알아보자.

### Shortest Paths Tree

버텍스마다 경로를 반환하는 단순한 방법으로는 각 경로가 Omega(|V|)의 크기를 가지기 때문에 Omega(|V|^2)의 시간이 소요된다.

대신에 parant P(v)만 저장한다. 최단 경로에서 직전에 오는 버텍스.

P(v)의 집합은 shortest paths tree를 구성한다.

D까지의 최단 경로 A -> B -> C -> D 는 B와 C의 최단경로도 포함한다. 최단 경로가 아니면 A -> C나 부분을 그걸로 대체하면 D까지의 새로운 최단 경로가 나온다는 식으로 증명 가능.

이렇게 자료 구조를 구성하면 O(|V|) 크기의 shortest paths tree가 생긴다.

### Breadth-First Search

Idea! Explore graph nodes in increasing order of distance

Goal: Compute level sets Li = {v | v ∈ V and d(s, v) = i} (i.e., all vertices at distance i)

```py
def bfs(Adj, s):
    # O(|V|)
    parent = [None for _ in Adj]
    parent[s] = s
    level = [[s]]
    while 0 < len(level[-1]):
        level.append([])
        # repeated at most O(SIGMAv∈V deg(v)) = O(|E|) times.
        for u in level[-2]:
            for v in Adj[u]:
                if parent[v] is None:
                    parent[v] = u
                    level[-1].append(v)
    return parent


def unweighted_shortest_path(Adj, s, t):
    parent = bfs(Adj, s)
    if parent[t] is None:
        return None
    i = t
    path = [t]
    while i != s:
        i = parent[i]
        path.append(i)
    return path[::-1]


Adj = [[1, 4, 3], [0], [3], [0, 2], [0]]

print(unweighted_shortest_path(Adj, 0, 2))
```

> Given an unweighted graph G = (V, E), find a shortest path from s to t having an odd number of edges.

입력 크기에 리니어한 시간 복잡도를 가진다. O(|V| + |E|).

## Quiz 1 review

How to solve computational problem?

- Design new algorithm from scratch(Generally hard thing to do).
- Reduce to known thing

자료 구조 문제에서는 operation 별 시간 복잡도 값과 그 관계가 중요해서 시간 복잡도가 명시되는 듯. 알고리즘 문제에서는 as fast as possible이 가능하다.

Solve first, optimize later.

해시 테이블에서 lookup하고 AVL tree에서 insert를 하는 경우 O(1)e + O(logn)이고 이 경우 worst case는 O(n)이고 expected 는 O(logn)e이다(because worst case can be higher).

Binary heap에서 amortization 없는 버전을 Sequence AVL tree의 augmentation로도 구현할 수 있다?? build를 빼고 말한거인듯?

## Problem Set 4

PDF 참조. 뒷부분 급발진해서 일반 패스,,,

## 10. Depth-First Search

A breadth-first search discovers vertices reachable from a queried vertex s level-by-level outward from s. A depth-first search (DFS) also finds all vertices reachable from s, but does so by search- ing undiscovered vertices as deep as possible before exploring other branches.

```py
def dfs(Adj, s, parent=None, order=None):
    if parent is None:
        parent = [None for v in Adj]  # O(V), use hash if unlabeled
        parent[s] = s
        order = []
    for v in Adj[s]:
        if parent[v] is None:
            parent[v] = s
            dfs(Adj, v, parent, order)
    order.append(s)
    return parent, order
```

### 증명

Claim: DFS visits v and correctly sets P (v) for every vertex v reachable from s

Proof: induct on k, for claim on only vertices within distance k from s

- Base case (k = 0): P (s) is set correctly for s and s is visited
- Inductive step: Consider vertex v with δ(s, v) = k0 + 1
- Consider vertex u, the second to last vertex on some shortest path from s to v
- By induction, since δ(s, u) = k0, DFS visits u and sets P (u) correctly
- While visiting u, DFS considers v ∈ Adj(u)
- Either v is in P , so has already been visited, or v will be visited while visiting u – In either case, v will be visited by DFS and will be added correctly to P

### 시간복잡도

BFS와 다르게 각 노드별 거리를 반환하지 않기에 O(|E|)에 동작한다? V에 속한 v에 대해서 deg(v)의 총합이다.

parent array가 길이 |V|이므로 O(|V|+|E|)이다? Lecture note랑 recitation이랑 말이 다른디,,, 재귀 호출로 한 일의 총합이 O(|E|)이라는 뜻인가.

### Full-BFS, Full-DFS

모든 노드들을 방문할 때까지 방문되지 않는 노드에 대해 그래프 탐색 알고리즘 A를 수행한다.

```py
from dfs import dfs


def full_dfs(Adj):
    parent = [None for _ in Adj]
    order = []
    for v in range(len(Adj)):
        if parent[v] is None:
            parent[v] = v
            dfs(Adj, v, parent, order)
    return parent, order
```

Such a search is conceptually equivalent to adding an auxiliary vertex with an outgoing edge to every vertex in the graph and then running breadth-first or depth-first search from the added vertex.

역사적인 이유(위상 정렬과 연결되어있는 것이 주 이유)로 dfs가 특정 노드에서부터 그래프를 탐색할때나 그래프 전체를 탐색할 때 주로 사용된다.

### DFS 간선 분류

증명에 유용하다. u에서 v로 가는 간선에서 간선이 DFS 트리를 구성하면 tree edge, 이외의 경우에 u가 v의 후손이면 back edge, v가 u의 후손이면 forward edge, 나머지는 cross edge라 한다.

### Graph Connectivity

무향그래프의 모든 노드쌍들을 연결하는 경로가 있으면 해당 무향그래프를 연결되었다고 한다. 유향그래프에서는 더 복잡한데 이 수업에서는 다루지 않는다.

> Connected Components(G): given undirected graph G = (V, E), return partition of V into subsets Vi ⊆ V (connected components) where each Vi is connected in G and there are no edges between vertices from different connected components

Single Source Rechability 문제를 푸는 알고리즘 A로 Full-A를 돌려 connected component들을 구할 수 있다.

### 위상 정렬(Topological Sort)

> A Directed Acyclic Graph (DAG) is a directed graph that contains no directed cycle.

> A topological sort of a directed acyclic graph G = (V, E) is a linear ordering of the vertices such that for each edge (u, v) in E, vertex u appears before vertex v in the ordering.

Claim: If G = (V, E) is a DAG, the reverse of a finishing order is a topological order

Proof: Need to prove, for every edge (u, v) ∈ E that u is ordered before v, i.e., the visit to v finishes before visiting u. Two cases:

– If u visited before v:

∗ Before visit to u finishes, will visit v (via (u, v) or otherwise) ∗ Thus the visit to v finishes before visiting u

– If v visited before u:

∗ u can’t be reached from v since graph is acyclic ∗ Thus the visit to v finishes before visiting u

### 사이클 탐색

Full-DFS의 reverse finish order가 위상 순서가 아니면 사이클이 존재한다. 해시 테이블이나 direct access memory를 사용해 O(|E|) 시간에 가능하다.

정확한 사이클을 반환하려면 Full-DPS 중에 경로의 ancestor들을 저장해놓는다.

• Proof: Consider a cycle (v0,v1,...,vk,v0) in G

- Without loss of generality, let v0 be the first vertex visited by Full-DFS on the cycle
- For each vi, before visit to vi finishes, will visit vi+1 and finish
- Will consider edge (vi, vi+1), and if vi+1 has not been visited, it will be visited now
- Thus, before visit to v0 finishes, will visit vk (for the first time, by v0 assumption)
- So, before visit to vk finishes, will consider (vk , v0), where v0 is an ancestor of vk

## 11. Weighted Shortest Paths

### 리뷰

Single Source Shortest Paths는 모든 노드에 대해서 거리를 반환해야하니까 O(|V| + |E|).

Single Source Rechability에서는 singleton인 노드는 신경 안쓰니까 O(|E|).

Connected components는 Full-BFS/Full-DFS를 사용해 O(|V| + |E|).

DAG의 위상 정렬은 Full-DFS를 사용해 O(|V| + |E|)

지금까지는 거리가 즉 경로에 있는 엣지의 개수였지만 경로의 개념을 보다 일반화해보자.

### Weighted Graphs

> A weighted graph is a graph G = (V, E) together with a weight function w : E → Z

Adjacency lists에 weight를 함께 저장하거나 별도의 set 자료구조를 사용할 수 있다.

> The weight w(π) of a path π in a weighted graph is the sum of weights of edges in the path

> The (weighted) shortest path from s ∈ V to t ∈ V is path of minimum weight from s to t

> δ(s, t) = inf{w(π) | path π from s to t} is the shortest-path weight from s to t

유한한 길이의 최소 weight path가 없을 수도 있어서 infimum을 사용한다. 경로의 합이 음수인 사이클이 있을 수도 있음.

### Weighted Shortest Paths Algorithms

다음 네 강의동안 weighted graph에서 최단 경로를 구하는 알고리즘을 살펴볼 것이다.

간선들이 모두 같은 weight를 가지거나 간선들의 합이 O(|V|+|E|)이어서 weight가 1인 간선들로 환원시킬 수 있는 제한적인 경우에는 BFS를 사용할 수 있다.

일반적인 weighted graph에서는 O(|V|+|E|)에 SSSP를 구하는 법은 모르지만 DAG의 경우는 안다. DAG는 좋은거다. 열심히 exploit해야함. 일반적인 그래프에서는 음수 사이클이 있을 수도 있어서 골치아픔.

- General/Unweighted: BFS, |V|+|E|
- DAG/Any: DAG Relaxation, |V|+|E|
- General/Any: Bellman-Ford, |V|\*|E|
- General/Non-negative: Dijkstra, |V|log|V| + |E|

DAG 간선들 전부 더해서 Non-negative로 만들고 다익스트라쓰면 더 이득 아닌가?!

모든 노드들에 대해 최단 거리를 안다면 이로부터 O(|V|+|E|)시간에 shortest-path tree를 만들어낼 수 있다. 그러니 거리만 신경쓰고 tree는 나중에 만들자. 아래는 최단 거리로부터 shortest-path tree 만드는 법.

- 모든 노드 u에 대해 해당 노드의 outgoing neighbor v에 대해 v까지의 거리가 유한하고, P(v)가 할당되지 않았으며, δ(s, v) = δ(s, u) + w(u, v)면 P(v) = u
- 부모 포인터들이 weight가 0인 사이클을 이룰 수 있음??? 이 경우 사이클 내부의 모든 노드들을 체크해놓는다.
- 체크안된 노드 u에 대해 u의 outgoing neightbor v에 대해 v가 체크되어있고 δ(s, v) = δ(s, u) + w(u, v)이면, v로부터 parent pointer들을 순회해 v를 포함하는 사이클의 노드들을 모두 체크 해제하고 P(v) = u로 해 사이클을 끊는다.

### DAG Releaxation

> As a general algorithmic paradigm, a relaxation algorithm searches for a solution to an optimization problem by starting with a solution that is not optimal, then iteratively improves the solution until it becomes an optimal solution to the original problem.

모든 노드들에 대한 예상 최단거리인 d(s, v)를 실제 거리의 upper bound로 잡아놓고 실제 거리와 같아질 때까지 triangle inequality를 통해 지속적으로 줄여보자.

> Triangle Inequality: the shortest-path weight from u to v cannot be greater than the shortest path from u to v through another vertex x, i.e., δ(u,v) ≤ δ(u,x)+δ(x,v) for all u,v,x ∈ V

방법: 모든 노드 v에 대하여 d(s, v)를 INF로 설정하고, 위상 정렬된 순서로 노드 u를 순회하며 outgoing neighbor v에 대해 triangle inequality가 위반된 경우 d(s, v)를 업데이트한다.

Relaxation이 안전함을 증명해보자. Relaxation을 통해 최단 경로를 업데이트해나가면 이 예측값이 실제값보다 작아질 수 없음을 증명해보자. **Safety Lemma**. d(s, v)는 경로의 길이이다. 무한으로 초기화하므로 초기값은 당연히 맞다. 이후 d(s, v')가 모든 v'에 대해 경로의 weight라고 하면 (u, v)를 통한 relaxing은 d(s, v)를 d(s, u) + w(u, v)로 설정한다. 즉, s에서 v를 u를 거쳐 가는 경로의 weight이다.

**Termination Lemma**. 모든 노드 v가 더이상 relax될 수 없을 때 d(s, v) ≤ δ(s, v)이다. 증명해보자. 역을 가정하보자. s에서 v까지 더 짧은 경로가 있고 이 중에는 d(b) > δ(s, b)인 간선이 있을 것이다. 이 간선을 통해 더 relax될 수 있으므로 모순이다.

s v a, b, v <- pi

증명: 위상 정렬을 했을 때 특정 노드는 자신 이전에 정렬된 노드들에 절대 방문할 수 없다. 초기값이 INF니 이들에 대해서는 증명됨. 처음 k'개의 노드에 대해 성립한다고 가정하자. v가 k'+1번째라 하고, u를 v로 가는 경로 이전에 있는 노드라 하자. s에서 v까지의 최단거리를 생각해보자. 가정에 따라서 d(s, v) = δ(s, u)이고 u를 처리할 때 d(s, v)는 triangle inequality에 따라 δ(s, u) + w(u, v) = δ(s, v) 보다 크지 않은 값으로 설정된다. 하지만 위에서 증명했듯 relaxation은 safe하기에 d(s, v) ≥ δ(s, v)이고 결국 두 값은 같다.

다른 증명: 어떠한 노드 v에 대해서 DAG relaxation은 d(s, v)를 v로 들어오는 노드 u에 대해서 d(s, u) + w(u, v)들의 최솟값으로 설정한다. v로의 최단 경로는 u중에 하나를 반드시 방문해야하니 맞다.

시간 복잡도는 O(|V| + |E|). 리니어하다.

### Exponential Relaxation

Relaxation을 재수없게 하면 지수 시간이 소요된다. 어떻게 다항 시간에 할까?

DAG에서는 위상 정렬된 순서로 간선들을 업데이트해나가면 최단 경로를 구할 수 있다. 이러한 알고리즘을 DAG Relaxation이라 한다.

```py
def DAG_Relaxation(Adj, w, s):
    _, order = dfs(Adj, s)
    order.reverse()
    d = [float('inf') for _ in Adj]
    parent = [None for _ in Adj]
    d[s], parent[s] = 0, s
    for u in order:
        for v in Adj[i]:
            try_to_relax(Adj, w, d, parent, u, v)
    return d, parent
```

Proof. We prove that at termination, d(s, v) = δ(s, v) for all v ∈ V . First observe that Safety ensures that a vertex not reachable from s will retain d(s,v) = +∞ at termination. Alterna- tively, consider any shortest path π = (v1, . . . , vm) from v1 = s to any vertex vm = v reach- able from s. The topological sort order ensures that edges of the path are relaxed in the order in which they appear in the path. Assume for induction that before edge (vi,vi+1) ∈ π is relaxed, d(s,vi) = δ(s,vi). Setting d(s,s) = 0 at the start provides a base case. Then relaxing edge (vi, vi+1) sets d(s, vi+1) = δ(s, vi) + w(vi, vi+1) = δ(s, vi+1), as sub-paths of shortest paths are also shortest paths. Thus the procedure constructs shortest path weights as desired. Since depth- first search runs in linear time and the loops relax each edge exactly once, this algorithm takes O(|V | + |E|) time.

### 기타

[What is the time complexity of array initialization?](https://stackoverflow.com/questions/16064820/what-is-the-time-complexity-of-array-initialization)

[Infimum and supremum](https://en.wikipedia.org/wiki/Infimum_and_supremum)
