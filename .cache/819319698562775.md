
[friedly ai](https://friendli.ai)

GPU -> only dynamic allocation?

Pytorch: a Python tensor library with GPU acceleration/audomatic differentiation etc...NumPy with GPU support.

```py
# explicit synchronization
touch.cuda.synchronize()
# 사실 print, memory allocation과 같이 기기간 transfer가 있으면 implicit하게 해준다.
# 대부분의 경우 신경쓰지 않아도 됨.
```

nvidia-smi definition: Percent of time over the past sample period during which **one or more kernels** was executing on the GPU.

배치 사이즈를 늘리기

[CUDA 기반의 GPU kernel 이해하기](https://m.blog.naver.com/julie_eun1014/221116294215)

https://cvw.cac.cornell.edu/GPUarch/kernel_sm

TorchScript.

Caching memory allocator. CUDA alloc / free != PyTorch alloc / free. CUDA alloc free가 느리기 떄문. 

```py
# deallocate된 x는 결국에 free된다
# 이전에 계산한 결과를 버리고 GPU 메모리를 아낄 수 있다?
# 역전파를 위해 추가적인 동작들이 있다?
x = conv1(x)
x = conv2(x)
x = fc(x)
print(x)
```

Backward prevents tensor free. Backward를 할 수도 있기 때문에 forward에서 준비, 저장을 하게 된다. no_grad flag를 달면 토치가 알고 저장해놓지 않는다.

touch.nn 패키지. module과 parameter 클래스가 있다. 
